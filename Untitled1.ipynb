{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05597e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "import keras\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: during this project I've changed my coding style\n",
    "# and was too lazy to edit the old code to match the new style\n",
    "# so please ignore any style related wierdness\n",
    "# thanks for not being petty about unimportant shit\n",
    "\n",
    "# ALSO NOTE: prints are for logging purposes\n",
    "\n",
    "#%% helper functions\n",
    "\n",
    "\n",
    "def dict2bin(row_inds_spike_times_map, num_segments, sim_duration_ms):\n",
    "    \n",
    "    bin_spikes_matrix = np.zeros((num_segments, sim_duration_ms), dtype='bool')\n",
    "    for row_ind in row_inds_spike_times_map.keys():\n",
    "        for spike_time in row_inds_spike_times_map[row_ind]:\n",
    "            bin_spikes_matrix[row_ind,spike_time] = 1.0\n",
    "    \n",
    "    return bin_spikes_matrix\n",
    "\n",
    "\n",
    "def parse_sim_experiment_file(sim_experiment_file):\n",
    "    \n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(\"loading file: '\" + sim_experiment_file.split(\"\\\\\")[-1] + \"'\")\n",
    "    loading_start_time = time.time()\n",
    "    experiment_dict = pickle.load(open(sim_experiment_file, \"rb\" ), encoding=\"latin1\")\n",
    "    \n",
    "    # gather params\n",
    "    num_simulations = len(experiment_dict['Results']['listOfSingleSimulationDicts'])\n",
    "    num_segments    = len(experiment_dict['Params']['allSegmentsType'])\n",
    "    sim_duration_ms = experiment_dict['Params']['totalSimDurationInSec'] * 1000\n",
    "    num_ex_synapses  = num_segments\n",
    "    num_inh_synapses = num_segments\n",
    "    num_synapses = num_ex_synapses + num_inh_synapses\n",
    "    \n",
    "    # collect X, y_spike, y_soma\n",
    "    X = np.zeros((num_synapses,sim_duration_ms,num_simulations), dtype='bool')\n",
    "    y_spike = np.zeros((sim_duration_ms,num_simulations))\n",
    "    y_soma  = np.zeros((sim_duration_ms,num_simulations))\n",
    "    for k, sim_dict in enumerate(experiment_dict['Results']['listOfSingleSimulationDicts']):\n",
    "        X_ex  = dict2bin(sim_dict['exInputSpikeTimes'] , num_segments, sim_duration_ms)\n",
    "        X_inh = dict2bin(sim_dict['inhInputSpikeTimes'], num_segments, sim_duration_ms)\n",
    "        X[:,:,k] = np.vstack((X_ex,X_inh))\n",
    "        spike_times = (sim_dict['outputSpikeTimes'].astype(float) - 0.5).astype(int)\n",
    "        y_spike[spike_times,k] = 1.0\n",
    "        y_soma[:,k] = sim_dict['somaVoltageLowRes']\n",
    "\n",
    "    loading_duration_sec = time.time() - loading_start_time\n",
    "    print('loading took %.3f seconds' %(loading_duration_sec))\n",
    "    print('-----------------------------------------------------------------')\n",
    "\n",
    "    return X, y_spike, y_soma\n",
    "\n",
    "\n",
    "def parse_multiple_sim_experiment_files(sim_experiment_files):\n",
    "    \n",
    "    for k, sim_experiment_file in enumerate(sim_experiment_files):\n",
    "        X_curr, y_spike_curr, y_soma_curr = parse_sim_experiment_file(sim_experiment_file)\n",
    "        \n",
    "        if k == 0:\n",
    "            X       = X_curr\n",
    "            y_spike = y_spike_curr\n",
    "            y_soma  = y_soma_curr\n",
    "        else:\n",
    "            X       = np.dstack((X,X_curr))\n",
    "            y_spike = np.hstack((y_spike,y_spike_curr))\n",
    "            y_soma  = np.hstack((y_soma,y_soma_curr))\n",
    "\n",
    "    return X, y_spike, y_soma\n",
    "\n",
    "\n",
    "def calc_AUC_at_desired_FP(y_test, y_test_hat, desired_false_positive_rate=0.01):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test.ravel(), y_test_hat.ravel())\n",
    "\n",
    "    linear_spaced_FPR = np.linspace(0,1,num=20000)\n",
    "    linear_spaced_TPR = np.interp(linear_spaced_FPR, fpr, tpr)\n",
    "    \n",
    "    desired_fp_ind = min(max(1, np.argmin(abs(linear_spaced_FPR - desired_false_positive_rate))), linear_spaced_TPR.shape[0] -1)\n",
    "    \n",
    "    return linear_spaced_TPR[:desired_fp_ind].mean()\n",
    "\n",
    "\n",
    "def calc_TP_at_desired_FP(y_test, y_test_hat, desired_false_positive_rate=0.0025):\n",
    "    print(y_test.shape, y_test_hat.shape)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test.ravel(), y_test_hat.ravel())\n",
    "    \n",
    "    desired_fp_ind = np.argmin(abs(fpr - desired_false_positive_rate))\n",
    "    if desired_fp_ind == 0:\n",
    "        desired_fp_ind = 1\n",
    "\n",
    "    return tpr[desired_fp_ind]\n",
    "\n",
    "\n",
    "def exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat, desired_FP_list=[0.0025,0.0100]):\n",
    "    # store results in the hyper param dict and return it\n",
    "    evaluations_results_dict = {}\n",
    "    \n",
    "    for desired_FP in desired_FP_list:\n",
    "        TP_at_desired_FP  = calc_TP_at_desired_FP(y_spikes_GT, y_spikes_hat, desired_false_positive_rate=desired_FP)\n",
    "        AUC_at_desired_FP = calc_AUC_at_desired_FP(y_spikes_GT, y_spikes_hat, desired_false_positive_rate=desired_FP\n",
    "        TP_key_string = 'TP @ %.4f FP' %(desired_FP)\n",
    "        evaluations_results_dict[TP_key_string] = TP_at_desired_FP\n",
    "    \n",
    "        AUC_key_string = 'AUC @ %.4f FP' %(desired_FP)\n",
    "        evaluations_results_dict[AUC_key_string] = AUC_at_desired_FP\n",
    "    \n",
    "    print('--------------------------------------------------')\n",
    "    fpr, tpr, thresholds = roc_curve(y_spikes_GT.ravel(), y_spikes_hat.ravel())\n",
    "    AUC_score = auc(fpr, tpr)\n",
    "    print('AUC = %.4f' %(AUC_score))\n",
    "    print('--------------------------------------------------')\n",
    "\n",
    "    evaluations_results_dict['AUC'] = AUC_score\n",
    "    return evaluations_results_dict\n",
    "\n",
    "\n",
    "def filter_and_exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat, desired_FP_list=[0.0025,0.0100],\n",
    "                                    ignore_time_at_start_ms=500, num_spikes_per_sim=[0,24]):\n",
    "\n",
    "    time_points_to_eval = np.arange(y_spikes_GT.shape[1]) >= ignore_time_at_start_ms\n",
    "    simulations_to_eval = np.logical_and((y_spikes_GT.sum(axis=1) >= num_spikes_per_sim[0]),(y_spikes_GT.sum(axis=1) <= num_spikes_per_sim[1]))\n",
    "    simulations_to_eval = simulations_to_eval.flatten()\n",
    "    print('total amount of simualtions is %d' %(y_spikes_GT.shape[0]))\n",
    "    print('percent of simulations kept = %.2f%s' %(100 * simulations_to_eval.mean(),'%'))\n",
    "    print(\"spikes gt\", y_spikes_GT.shape, y_soma_GT.shape)\n",
    "    print(\"to eval time points\", simulations_to_eval.shape, time_points_to_eval.shape)\n",
    "    y_spikes_GT_to_eval  = y_spikes_GT[simulations_to_eval, :][:, time_points_to_eval]\n",
    "    y_spikes_hat_to_eval = y_spikes_hat[simulations_to_eval, :][:, time_points_to_eval]\n",
    "    y_soma_GT_to_eval    = y_soma_GT[simulations_to_eval, :][:, time_points_to_eval]\n",
    "    y_soma_hat_to_eval   = y_soma_hat[simulations_to_eval, :][:, time_points_to_eval]\n",
    "    if any(i == 0 for i in y_spikes_GT_to_eval.shape) or any(i == 0 for i in y_spikes_hat_to_eval.shape) or\\\n",
    "        any(i == 0 for i in y_soma_GT_to_eval.shape) or any(i == 0 for i in y_soma_hat_to_eval.shape):\n",
    "        return 0\n",
    "    return exctract_key_results(y_spikes_GT_to_eval, y_spikes_hat_to_eval, y_soma_GT_to_eval, y_soma_hat_to_eval, desired_FP_list=desired_FP_list)\n",
    "\n",
    "\n",
    "def load_experiment_file(fpath: str):\n",
    "    test_file_loading_start_time = time.time()\n",
    "    v_threshold = -55\n",
    "    # load data\n",
    "    X_test, y_spike_test, y_soma_test = \\\n",
    "        parse_multiple_sim_experiment_files([fpath])\n",
    "    y_soma_test[y_soma_test > v_threshold] = v_threshold\n",
    "    \n",
    "    y_train_soma_bias = -67.7\n",
    "\n",
    "    X_test = np.transpose(X_test,axes=[2,1,0])\n",
    "    y_spike_test = y_spike_test.T[:,:,np.newaxis]\n",
    "    y_soma_test = y_soma_test.T[:,:,np.newaxis] - y_train_soma_bias\n",
    "\n",
    "    \n",
    "    test_file_loading_duration_min = (time.time() - test_file_loading_start_time) / 60\n",
    "    print('time took to load data is %.3f minutes' %(test_file_loading_duration_min))\n",
    "    return X_test, (y_spike_test, y_soma_test)\n",
    "\n",
    "\n",
    "def read_model(model_filename, metadata_filename):\n",
    "    model_loading_start_time = time.time()\n",
    "\n",
    "    model = load_model(model_filename)\n",
    "\n",
    "    input_window_size = model.input_shape[1]\n",
    "\n",
    "    # load pickle file\n",
    "    model_metadata = pickle.load(open(metadata_filename, \"rb\" ),\n",
    "                                      encoding=\"latin1\")\n",
    "    time_window = (np.array(model_metadata[\"architecture_dict\"]['filter_sizes_per_layer'])- 1).sum() + 1\n",
    "    overlap_size = min(max(time_window + 1, min(150, input_window_size - 50)), 250)\n",
    "    model_loading_duration_min = (time.time() - model_loading_start_time) / 60\n",
    "    print('overlap_size = %d' %(overlap_size))\n",
    "    print('time_window_T = %d' %(time_window))\n",
    "    print('time took to load model is %.3f minutes' %(model_loading_duration_min))\n",
    "    return model, {\"input_window_size\": input_window_size,\n",
    "                   \"time_window\": time_window,\n",
    "                   \"overlap_size\": overlap_size}\n",
    "\n",
    "def apply_model(model, params, X_test_for_keras, y_spike_test, y_soma_test):\n",
    "    # load pickle file\n",
    "    input_window_size = params[\"input_window_size\"]\n",
    "    time_window = params[\"time_window\"]\n",
    "    overlap_size = params[\"overlap_size\"]\n",
    "\n",
    "    y1_test_for_keras_hat = np.zeros(y_spike_test.shape)\n",
    "    y2_test_for_keras_hat = np.zeros(y_soma_test.shape)\n",
    "\n",
    "    num_test_splits = 2 + (X_test_for_keras.shape[1] - input_window_size) // (input_window_size - overlap_size)\n",
    "\n",
    "    for k in range(num_test_splits):\n",
    "        print(k, num_test_splits)\n",
    "        start_time_ind = k * (input_window_size - overlap_size)\n",
    "        end_time_ind   = start_time_ind + input_window_size\n",
    "\n",
    "        curr_X_test_for_keras = X_test_for_keras[:,start_time_ind:end_time_ind,:]\n",
    "\n",
    "        if curr_X_test_for_keras.shape[1] < input_window_size:\n",
    "            padding_size = input_window_size - curr_X_test_for_keras.shape[1]\n",
    "            X_pad = np.zeros((curr_X_test_for_keras.shape[0],padding_size,curr_X_test_for_keras.shape[2]))\n",
    "            curr_X_test_for_keras = np.hstack((curr_X_test_for_keras,X_pad))\n",
    "\n",
    "        curr_y1_test_for_keras, curr_y2_test_for_keras, _ = model.predict(curr_X_test_for_keras)\n",
    "\n",
    "        if k == 0:\n",
    "            y1_test_for_keras_hat[:,:end_time_ind,:] = curr_y1_test_for_keras\n",
    "            y2_test_for_keras_hat[:,:end_time_ind,:] = curr_y2_test_for_keras\n",
    "        elif k == (num_test_splits - 1):\n",
    "            t0 = start_time_ind + overlap_size\n",
    "            duration_to_fill = y1_test_for_keras_hat.shape[1] - t0\n",
    "            y1_test_for_keras_hat[:,t0:,:] = curr_y1_test_for_keras[:,overlap_size:(overlap_size + duration_to_fill),:]\n",
    "            y2_test_for_keras_hat[:,t0:,:] = curr_y2_test_for_keras[:,overlap_size:(overlap_size + duration_to_fill),:]\n",
    "        else:\n",
    "            t0 = start_time_ind + overlap_size\n",
    "            y1_test_for_keras_hat[:,t0:end_time_ind,:] = curr_y1_test_for_keras[:,overlap_size:,:]\n",
    "            y2_test_for_keras_hat[:,t0:end_time_ind,:] = curr_y2_test_for_keras[:,overlap_size:,:]\n",
    "\n",
    "    # zero score the prediction and align it with the actual test\n",
    "    s_dst = y_soma_test.std()\n",
    "    m_dst = y_soma_test.mean()\n",
    "\n",
    "    s_src = y2_test_for_keras_hat.std()\n",
    "    m_src = y2_test_for_keras_hat.mean()\n",
    "\n",
    "    y2_test_for_keras_hat = (y2_test_for_keras_hat - m_src) / s_src\n",
    "    y2_test_for_keras_hat = s_dst * y2_test_for_keras_hat + m_dst\n",
    "\n",
    "    y_test = y_spike_test\n",
    "    y_test_hat = y1_test_for_keras_hat[:,:,0].T\n",
    "\n",
    "    # convert to simple (num_sims X num_time_points) format\n",
    "    y_spikes_hat = y_test_hat.T\n",
    "    y_soma_hat   = y2_test_for_keras_hat[:,:,0]\n",
    "    return y_spikes_hat, y_soma_hat\n",
    "\n",
    "\n",
    "#%% loop through all models files, make prediction on valid set, evaluate perfomrance and store\n",
    "def evaluate_model(model, params, X_test, y_spike_test, y_soma_test, save_fname):\n",
    "    prediction_start_time = time.time()\n",
    "    \n",
    "    y_spikes_hat, y_soma_hat = apply_model(model, params, X_test, y_spike_test, y_soma_test)\n",
    "    y_spikes_GT  = y_spike_test\n",
    "    y_soma_GT    = y_soma_test\n",
    "\n",
    "    prediction_duration_min = (time.time() - prediction_start_time) / 60\n",
    "    print('finished prediction. time took to predict is %.2f minutes' %(prediction_duration_min))\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # evaluate the model and save the results\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    print('calculating and saving key results...')\n",
    "\n",
    "    saving_start_time = time.time()\n",
    "\n",
    "    desired_FP_list = [0.0001, 0.0005, 0.0010, 0.0015, 0.0020, 0.0025, 0.0050, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.1000]\n",
    "    evaluations_results_dict = {}\n",
    "\n",
    "    # (1) we ignore the first 500 ms so that the simulation will \"settle\" and forget about any initial conditions\n",
    "    # (2) we control for the number of spikes per simulation in order to make proper comparisons for different biophysical models (AMPA,NMDA,AMPA_SK, etc.)\n",
    "    # this is due to the fact the the number of output spikes greatley changes the results and it's important to keep a tight control on it\n",
    "\n",
    "    ignore_time_at_start_ms = 500\n",
    "    num_spikes_per_sim = [0,18]\n",
    "    filter_string = 'starting_at_%dms_spikes_in_[%d,%d]_range' %(ignore_time_at_start_ms, num_spikes_per_sim[0], num_spikes_per_sim[1])\n",
    "    evaluations_results_dict[filter_string] = filter_and_exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat,\n",
    "                                                                              desired_FP_list=desired_FP_list,\n",
    "                                                                              ignore_time_at_start_ms=ignore_time_at_start_ms,\n",
    "                                                                              num_spikes_per_sim=num_spikes_per_sim)\n",
    "\n",
    "    ignore_time_at_start_ms = 500\n",
    "    num_spikes_per_sim = [1,24]\n",
    "    filter_string = 'starting_at_%dms_spikes_in_[%d,%d]_range' %(ignore_time_at_start_ms, num_spikes_per_sim[0], num_spikes_per_sim[1])\n",
    "    evaluations_results_dict[filter_string] = filter_and_exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat,\n",
    "                                                                              desired_FP_list=desired_FP_list,\n",
    "                                                                              ignore_time_at_start_ms=ignore_time_at_start_ms,\n",
    "                                                                              num_spikes_per_sim=num_spikes_per_sim)\n",
    "\n",
    "    ignore_time_at_start_ms = 500\n",
    "    num_spikes_per_sim = [0,24]\n",
    "    filter_string = 'starting_at_%dms_spikes_in_[%d,%d]_range' %(ignore_time_at_start_ms, num_spikes_per_sim[0], num_spikes_per_sim[1])\n",
    "    evaluations_results_dict[filter_string] = filter_and_exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat,\n",
    "                                                                              desired_FP_list=desired_FP_list,\n",
    "                                                                              ignore_time_at_start_ms=ignore_time_at_start_ms,\n",
    "                                                                              num_spikes_per_sim=num_spikes_per_sim)\n",
    "\n",
    "    ignore_time_at_start_ms = 500\n",
    "    num_spikes_per_sim = [0,30]\n",
    "    filter_string = 'starting_at_%dms_spikes_in_[%d,%d]_range' %(ignore_time_at_start_ms, num_spikes_per_sim[0], num_spikes_per_sim[1])\n",
    "    evaluations_results_dict[filter_string] = filter_and_exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat,\n",
    "                                                                              desired_FP_list=desired_FP_list,\n",
    "                                                                              ignore_time_at_start_ms=ignore_time_at_start_ms,\n",
    "                                                                              num_spikes_per_sim=num_spikes_per_sim)\n",
    "\n",
    "    ignore_time_at_start_ms = 500\n",
    "    num_spikes_per_sim = [0,90]\n",
    "    filter_string = 'starting_at_%dms_spikes_in_[%d,%d]_range' %(ignore_time_at_start_ms, num_spikes_per_sim[0], num_spikes_per_sim[1])\n",
    "    evaluations_results_dict[filter_string] = filter_and_exctract_key_results(y_spikes_GT, y_spikes_hat, y_soma_GT, y_soma_hat,\n",
    "                                                                              desired_FP_list=desired_FP_list,\n",
    "                                                                              ignore_time_at_start_ms=ignore_time_at_start_ms,\n",
    "                                                                              num_spikes_per_sim=num_spikes_per_sim)\n",
    "\n",
    "    #model_metadata_dict['evaluations_results_dict'] = evaluations_results_dict\n",
    "    #print('saving:   \"%s\"' %(metadata_evaluation_filename))\n",
    "    #pickle.dump(model_metadata_dict, open(save_fname, \"wb\"), protocol=2)\n",
    "\n",
    "    saving_duration_min = (time.time() - saving_start_time) / 60\n",
    "    print('time took to evaluate and save results is %.3f minutes' %(saving_duration_min))\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    return evaluations_results_dict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
